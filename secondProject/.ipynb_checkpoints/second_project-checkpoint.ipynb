{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching data from imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_fetch():\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()\n",
    "\n",
    "    word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "    index2word = dict((i + 3, word) for (word, i) in word_index.items())\n",
    "    index2word[0] = '[pad]' #padding\n",
    "    index2word[1] = '[bos]' #begin of sentence\n",
    "    index2word[2] = '[oov]' # out of vocabulary\n",
    "    x_train = np.array([' '.join([index2word[idx] for idx in text]) for text in x_train])\n",
    "    x_test = np.array([' '.join([index2word[idx] for idx in text]) for text in x_test])\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_examples(vocabulary, x_train):\n",
    "    binary_vectorizer = CountVectorizer(binary=True, vocabulary=vocabulary.keys())\n",
    "    x_train_binary = binary_vectorizer.fit_transform(x_train)\n",
    "    x_train_binary = x_train_binary.toarray()\n",
    "    return x_train_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ig(classes_vector, feature):\n",
    "        classes = set(classes_vector)\n",
    "\n",
    "        HC = 0\n",
    "        for c in classes:\n",
    "            PC = list(classes_vector).count(c) / len(classes_vector)  # P(C=c)\n",
    "            HC += - PC * math.log(PC, 2)  # H(C)\n",
    "            # print('Overall Entropy:', HC)  # entropy for C variable\n",
    "\n",
    "        feature_values = set(feature)  # 0 or 1 in this example\n",
    "        HC_feature = 0\n",
    "        for value in feature_values:\n",
    "            # pf --> P(X=x)\n",
    "            pf = list(feature).count(value) / len(feature)  # count occurences of value \n",
    "            indices = [i for i in range(len(feature)) if feature[i] == value]  # rows (examples) that have X=x\n",
    "\n",
    "            classes_of_feat = [classes_vector[i] for i in indices]  # category of examples listed in indices above\n",
    "            for c in classes:\n",
    "                # pcf --> P(C=c|X=x)\n",
    "                pcf = classes_of_feat.count(c) / len(classes_of_feat)  # given X=x, count C\n",
    "                if pcf != 0: \n",
    "                    # - P(X=x) * P(C=c|X=x) * log2(P(C=c|X=x))\n",
    "                    temp_H = - pf * pcf * math.log(pcf, 2)\n",
    "                    # sum for all values of C (class) and X (values of specific feature)\n",
    "                    HC_feature += temp_H\n",
    "\n",
    "        ig = HC - HC_feature\n",
    "        return ig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(x_train, m, n, k, l):\n",
    "    # n most frequent\n",
    "    # k less frequent\n",
    "    # ig \n",
    "    \n",
    "    words_frequency_Dict = dict()\n",
    "\n",
    "    for review in x_train:\n",
    "        # I need a list with the distinct words of every review\n",
    "        distinct_words = set(review.split())\n",
    "\n",
    "        for word in distinct_words:\n",
    "            if word in words_frequency_Dict.keys():\n",
    "                words_frequency_Dict[word] += 1\n",
    "            else:\n",
    "                words_frequency_Dict[word] = 1\n",
    "    \n",
    "    words_frequency_Dict.pop('[bos]',' ')\n",
    "    words_frequency_Dict.pop('[pad]',' ')\n",
    "    words_frequency_Dict.pop('[oov]',' ')\n",
    "                \n",
    "    # Sort words based on their frequency in descending order\n",
    "    remaining_words = sorted(words_frequency_Dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "     # Exclude the top n and bottom k words\n",
    "    remaining_words = remaining_words[n:-k] if k > 0 else remaining_words[n:]\n",
    "\n",
    "    #create new dictionary which shows the IG\n",
    "    IG_Dict = dict()\n",
    "    x_train_binary = vectorize_examples(remaining_words, x_train)\n",
    "    for i in tqdm(range(len(remaining_words))):\n",
    "        word = [example[i] for example in x_train_binary]\n",
    "        IG_Dict[list(remaining_words.keys())[i]] = IG(y_train, word)\n",
    "\n",
    "    remaining_words = sorted(IG_Dict.items(), key=lambda x: x[1])\n",
    "    remaining_words = remaining_words[:l] \n",
    "\n",
    "   # Convert remaining_words back into a dictionary\n",
    "    remaining_words_dict = dict(remaining_words)\n",
    "\n",
    "    return remaining_words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x_train, y_train, x_test, y_test \u001b[38;5;241m=\u001b[39m data_fetch()\n\u001b[0;32m----> 2\u001b[0m vocabulary \u001b[38;5;241m=\u001b[39m create_vocabulary(x_train, \u001b[38;5;241m2500\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m85000\u001b[39m, \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(vocabulary)\n",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m, in \u001b[0;36mcreate_vocabulary\u001b[0;34m(x_train, m, n, k, l)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#create new dictionary which shows the IG\u001b[39;00m\n\u001b[1;32m     29\u001b[0m IG_Dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m---> 30\u001b[0m x_train_binary \u001b[38;5;241m=\u001b[39m vectorize_examples(remaining_words, x_train)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(remaining_words))):\n\u001b[1;32m     32\u001b[0m     word \u001b[38;5;241m=\u001b[39m [example[i] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m x_train_binary]\n",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m, in \u001b[0;36mvectorize_examples\u001b[0;34m(vocabulary, x_train)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvectorize_examples\u001b[39m(vocabulary, x_train):\n\u001b[0;32m----> 2\u001b[0m     binary_vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, vocabulary\u001b[38;5;241m=\u001b[39mvocabulary\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      3\u001b[0m     x_train_binary \u001b[38;5;241m=\u001b[39m binary_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(x_train)\n\u001b[1;32m      4\u001b[0m     x_train_binary \u001b[38;5;241m=\u001b[39m x_train_binary\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = data_fetch()\n",
    "vocabulary = create_vocabulary(x_train, 2500, 50, 85000, 1000)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NaiveBayesCustom():\n",
    "\n",
    "#     def __init__(self):\n",
    "#         self.class0_prob = None\n",
    "#         self.class1_prob = None\n",
    "#         self.features_probs = None\n",
    "\n",
    "#     def fit(self,x_train_binary, y_train):\n",
    "\n",
    "#         # Calculate prior probabilites P(C=0) and P(C=1)\n",
    "#         total_samples = len(y_train)\n",
    "#         class0_samples = np.sum(y_train == 0)\n",
    "#         class0_prob = class0_samples / total_samples\n",
    "\n",
    "#         class1_prob = (total_samples - class0_samples) / total_samples\n",
    "\n",
    "#         # !!!!!!!!!!!!!!!!!!!!!!\n",
    "#         self.class0_prob = class0_prob\n",
    "#         self.class1_prob = class1_prob\n",
    "\n",
    "#         # Calculate the likelihood\n",
    "#         self.feature_probs = {}\n",
    "\n",
    "#         # Select samples belonging to class 0,1\n",
    "#         X_0 = x_train_binary[y == 0]\n",
    "#         X_1 = x_train_binary[y == 1]\n",
    "\n",
    "#         # Calculate the probability of each feature being 1 given the class\n",
    "#         # feature_probs_c = [np.mean(X_c[:, i]) \n",
    "#         for i in range(num_features)]\n",
    "        \n",
    "#         self.feature_probs[c] = feature_probs_c\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     def predict(self, x_test_binary):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
