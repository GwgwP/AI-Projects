{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Gwgw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching data from imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_fetch():\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()\n",
    "\n",
    "    word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "    index2word = dict((i + 3, word) for (word, i) in word_index.items())\n",
    "    index2word[0] = '[pad]' #padding\n",
    "    index2word[1] = '[bos]' #begin of sentence\n",
    "    index2word[2] = '[oov]' # out of vocabulary\n",
    "    x_train = np.array([' '.join([index2word[idx] for idx in text]) for text in x_train])\n",
    "    x_test = np.array([' '.join([index2word[idx] for idx in text]) for text in x_test])\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vectorize_examples(vocabulary, x_train):\n",
    "#     binary_vectorizer = CountVectorizer(binary=True, vocabulary=vocabulary.keys())\n",
    "#     x_train_binary = binary_vectorizer.fit_transform(x_train)\n",
    "#     x_train_binary = x_train_binary.toarray()\n",
    "#     return x_train_binary\n",
    "\n",
    "def vectorize_examples(vocabulary, x_train):\n",
    "    binary_vectorizer = CountVectorizer(binary=True, vocabulary=vocabulary.keys())\n",
    "    x_train_binary = binary_vectorizer.fit_transform(x_train)\n",
    "    x_train_binary = x_train_binary.toarray()\n",
    "    return x_train_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ig(classes_vector, feature):\n",
    "        classes = set(classes_vector)\n",
    "\n",
    "        HC = 0\n",
    "        for c in classes:\n",
    "            PC = list(classes_vector).count(c) / len(classes_vector)  # P(C=c)\n",
    "            HC += - PC * math.log(PC, 2)  # H(C)\n",
    "            # print('Overall Entropy:', HC)  # entropy for C variable\n",
    "\n",
    "        feature_values = set(feature)  # 0 or 1 in this example\n",
    "        HC_feature = 0\n",
    "        for value in feature_values:\n",
    "            # pf --> P(X=x)\n",
    "            pf = list(feature).count(value) / len(feature)  # count occurences of value \n",
    "            indices = [i for i in range(len(feature)) if feature[i] == value]  # rows (examples) that have X=x\n",
    "\n",
    "            classes_of_feat = [classes_vector[i] for i in indices]  # category of examples listed in indices above\n",
    "            for c in classes:\n",
    "                # pcf --> P(C=c|X=x)\n",
    "                pcf = classes_of_feat.count(c) / len(classes_of_feat)  # given X=x, count C\n",
    "                if pcf != 0: \n",
    "                    # - P(X=x) * P(C=c|X=x) * log2(P(C=c|X=x))\n",
    "                    temp_H = - pf * pcf * math.log(pcf, 2)\n",
    "                    # sum for all values of C (class) and X (values of specific feature)\n",
    "                    HC_feature += temp_H\n",
    "\n",
    "        ig = HC - HC_feature\n",
    "        return ig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_vocabulary(x_train,y_train, n, k, m, l):\n",
    "    words_frequency_dict = dict()\n",
    "\n",
    "    for review in x_train:\n",
    "        distinct_words = set(review.split())\n",
    "\n",
    "        for word in distinct_words:\n",
    "            if word in words_frequency_dict:\n",
    "                words_frequency_dict[word] += 1\n",
    "            else:\n",
    "                words_frequency_dict[word] = 1\n",
    "    \n",
    "    # Remove specific words from the dictionary\n",
    "    for special_word in ['[bos]', '[pad]', '[oov]']:\n",
    "        words_frequency_dict.pop(special_word, None)\n",
    "   \n",
    "\n",
    "\n",
    "    # Sort words based on their frequency in descending order\n",
    "    remaining_words = sorted(words_frequency_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    # Exclude the top n and bottom k words\n",
    "    remaining_words = remaining_words[n:-k] \n",
    "    # Convert remaining_words back into a dictionary\n",
    "    remaining_words = dict(remaining_words)\n",
    "\n",
    "    # Create a new dictionary which shows the IG\n",
    "    IG_dict = dict()\n",
    "    x_train_binary = vectorize_examples(remaining_words, x_train)\n",
    "    \n",
    "\n",
    "    for i in tqdm(range(len(remaining_words))):\n",
    "        # word = [example[i] for example in x_train_binary.T]\n",
    "        word = [example[i] for example in x_train_binary]\n",
    "        IG_dict[list(remaining_words.keys())[i]] = calculate_ig(y_train, word)\n",
    "\n",
    "    # Sort words based on Information Gain in ascending order\n",
    "    remaining_words = sorted(IG_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    # Select the top l words\n",
    "    remaining_words = remaining_words[:l] \n",
    "    # Convert remaining_words back into a dictionary\n",
    "    remaining_words_dict = dict(remaining_words)\n",
    "\n",
    "    return remaining_words_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3026/3026 [00:28<00:00, 104.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 1000)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = data_fetch()\n",
    "vocabulary = create_vocabulary(x_train,y_train, 50, 85000, 2500, 1000)\n",
    "x_train_binary = vectorize_examples(vocabulary, x_train)\n",
    "x_test_binary = vectorize_examples(vocabulary, x_test)\n",
    "print(x_train_binary.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesCustom():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.class0_prob = None\n",
    "        self.class1_prob = None\n",
    "        self.features_probs = None\n",
    "\n",
    "    def fit(self,x_train_binary, y_train):\n",
    "\n",
    "        # Calculate prior probabilites P(C=0) and P(C=1)\n",
    "        total_samples = len(y_train)\n",
    "        class0_samples = np.sum(y_train == 0)\n",
    "        class0_prob = class0_samples / total_samples\n",
    "        class1_prob = (total_samples - class0_samples) / total_samples\n",
    "\n",
    "        self.class0_prob = class0_prob\n",
    "        self.class1_prob = class1_prob\n",
    "\n",
    "        # Calculate the likelihood\n",
    "        self.feature_probs = np.zeros((2,x_train_binary.shape[1]))\n",
    "\n",
    "        # Select samples belonging to class 0,1\n",
    "        X_0 = []\n",
    "        X_1 = []  #alliws np.array\n",
    "\n",
    "        for i in range(x_train_binary.shape[1]):\n",
    "            if y_train[i] == 0:\n",
    "                X_0.append(x_train_binary[i])\n",
    "            else:\n",
    "                X_1.append(x_train_binary[i])\n",
    "            \n",
    "        # Convert lists to numpy arrays\n",
    "        X_0 = np.array(X_0)\n",
    "        X_1 = np.array(X_1)\n",
    "\n",
    "        # Calculate the probability of each feature being 0 given the class\n",
    "        self.feature_probs[0] = (X_0.sum(axis=0) + 1) / (len(X_0) + 2)   \n",
    "        # Calculate the probability of each feature being 1 given the class\n",
    "        self.feature_probs[1] = (X_1.sum(axis=0) + 1) / (len(X_1) + 2)   \n",
    "        \n",
    "\n",
    "    def predict(self, x_test_binary):\n",
    "        \n",
    "        sum_prob0=0\n",
    "        sum_prob1=0\n",
    "\n",
    "        num_features = x_test_binary.shape[1]\n",
    "        y_predict =[]\n",
    "\n",
    "        # Calculating P(C=1 | x_test_binary) and P(C=0 | x_test_binary)\n",
    "        \n",
    "        for x_test in x_test_binary:\n",
    "            # sum_prob0 = sum( math.log(self.feature_probs[0][i]) if x_test[i] == 1 else  math.log(1-self.feature_probs[0][i]) for i in range(num_features) )\n",
    "            # sum_prob1 = sum( math.log(self.feature_probs[1][i]) if x_test[i] == 1 else  math.log(1-self.feature_probs[1][i]) for i in range(num_features) )\n",
    "\n",
    "            # sum_prob1 = math.log(self.class1_prob) + sum_prob1\n",
    "            # sum_prob0 = math.log(self.class0_prob) + sum_prob0\n",
    "\n",
    "            feature_prob_0 = np.log(self.feature_probs[0])\n",
    "            feature_prob_1 = np.log(self.feature_probs[1])\n",
    "            feature_prob_0 = np.sum(feature_prob_0 * x_test + np.log(1 - np.exp(feature_prob_0) * x_test), axis=0)\n",
    "            feature_prob_1 = np.sum(feature_prob_1 * x_test + np.log(1 - np.exp(feature_prob_1) * x_test), axis=0)\n",
    "            sum_prob0 = np.log(self.class0_prob) + feature_prob_0\n",
    "            sum_prob1 = np.log(self.class1_prob) +feature_prob_1\n",
    "\n",
    "            if (sum_prob1 > sum_prob0):\n",
    "                y_predict.append(1)\n",
    "            elif(sum_prob1 < sum_prob0):\n",
    "                y_predict.append(0)\n",
    "            else:\n",
    "                y_predict.append(1 if self.class1_prob > self.class0_prob else 0)\n",
    "\n",
    "\n",
    "        return y_predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training And Testing - Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                    # 1. Custom Naive Bayes\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85     12500\n",
      "           1       0.87      0.81      0.84     12500\n",
      "\n",
      "    accuracy                           0.85     25000\n",
      "   macro avg       0.85      0.85      0.84     25000\n",
      "weighted avg       0.85      0.85      0.84     25000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.85     12500\n",
      "           1       0.87      0.80      0.83     12500\n",
      "\n",
      "    accuracy                           0.84     25000\n",
      "   macro avg       0.84      0.84      0.84     25000\n",
      "weighted avg       0.84      0.84      0.84     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nbc = NaiveBayesCustom()\n",
    "nbc.fit(x_train_binary, y_train)\n",
    "print(classification_report(y_train, nbc.predict(x_train_binary),zero_division=1))\n",
    "print(classification_report(y_test, nbc.predict(x_test_binary), zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                        # 2. Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84     12500\n",
      "           1       0.85      0.82      0.83     12500\n",
      "\n",
      "    accuracy                           0.84     25000\n",
      "   macro avg       0.84      0.84      0.84     25000\n",
      "weighted avg       0.84      0.84      0.84     25000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.84      0.82     12500\n",
      "           1       0.84      0.80      0.82     12500\n",
      "\n",
      "    accuracy                           0.82     25000\n",
      "   macro avg       0.82      0.82      0.82     25000\n",
      "weighted avg       0.82      0.82      0.82     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(x_train_binary, y_train)\n",
    "print(classification_report(y_train, nb.predict(x_train_binary),\n",
    "                            zero_division=1))\n",
    "print(classification_report(y_test, nb.predict(x_test_binary),\n",
    "                            zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLogisticRegression():\n",
    "    def __init__(self, regularizator, learning_rate, n_iterations):\n",
    "        self.regularizator = regularizator\n",
    "        self.learing_rate = learning_rate\n",
    "        self.num_iterations = n_iterations\n",
    "        self.weights = None\n",
    "\n",
    "    def sigmoid(self, t):\n",
    "        return 1 / (1 + np.exp(-t))\n",
    "    \n",
    "    def fit(self,x_train_binary, y_train):\n",
    "        \"\"\"\n",
    "        to stop the while loop, we need to calculate the s. this is the accuracy score. \n",
    "        to do so, we need to split our data into trainig and testing so every time \n",
    "        we can check the accuracy score.\n",
    "\n",
    "        epoch : while the accuracy score is not acceptable, start a new epoch iterating again\n",
    "                all the examples  (_x_train)\n",
    "        \"\"\"\n",
    "        _x_train, _x_valuation, _y_train, _y_valuation = train_test_split(x_train_binary, y_train, test_size=0.2)\n",
    "        \n",
    "        num_of_features = _x_train.shape[1]\n",
    "        num_of_examples = len(_y_train)\n",
    "\n",
    "        \"\"\" \n",
    "            w0*x0 -> x0 = 1 (bias term)                                                                        # or  _x_train = np.insert(_x_train, 0, np.ones(num_of_examples), axis =1)\n",
    "            Concatenate a column of ones to the left side of the entire array\n",
    "        \"\"\"\n",
    "        _x_train = np.c_[np.ones(num_of_examples), _x_train]\n",
    "        best_s=0\n",
    "\n",
    "        # step 1 : start with random weights\n",
    "        self.weights = np.random.rand(1, num_of_features+1) # num_of_features+1 beacuse we have the w0 (bias term)\n",
    "\n",
    "        iteration = 1 # epoch\n",
    "        unchanged_epochs = 0\n",
    "        best_iteration = 0\n",
    "        while ( iteration<=self.num_iterations):\n",
    "            # step 2 :  \n",
    "            #---------------SHUFFLE-------------\n",
    "            # Create a permutation index\n",
    "            permutation_index = np.random.permutation(len(_y_train))\n",
    "            # Use the permutation index to shuffle both arrays\n",
    "            shuffled_y_train = _y_train[permutation_index]\n",
    "            shuffled_x_train_binary = _x_train[permutation_index]\n",
    "            #---------------END SHUFFLE-------------\n",
    "\n",
    "            # maximizing l(w) \n",
    "            for i in range (num_of_examples):\n",
    "                # step 3: calculating li(w) for every example\n",
    "                \"\"\" \n",
    "                The np.dot function is used for matrix multiplication or dot product \n",
    "                between arrays. It calculates the sum of the element-wise products\n",
    "                of two arrays.\n",
    "                \"\"\"\n",
    "                t = np.dot(self.weights, shuffled_x_train_binary[i]) # t = w*x\n",
    "                p_c_positive = self.sigmoid(t) # P(c+|xi)\n",
    "\n",
    "                l = (shuffled_y_train[i]-p_c_positive)*shuffled_x_train_binary[i] # (yi - P(c+|xi)) * xi\n",
    "              \n",
    "                #step 5: inform weights\n",
    "                self.weights =(1-2*self.learing_rate*self.regularizator) * self.weights + self.learing_rate * l       \n",
    "                # step 6: move to next example \n",
    "\n",
    "            # checkig the accuracy \n",
    "            #step 7 :\n",
    "            s = accuracy_score(_y_valuation, self.predict(_x_valuation))\n",
    "            if s > best_s:\n",
    "                best_iteration = iteration \n",
    "                best_s = s\n",
    "                best_weights = self.weights\n",
    "                unchanged_epochs = 0\n",
    "            else:\n",
    "                iteration += 1 # increasing number of epoch\n",
    "                unchanged_epochs +=1\n",
    "\n",
    "            if unchanged_epochs == 33: # ανεχομαστε μεχρι 33 εποχες χωρις να εχουν βελτιωωθει γιατι μπορει να βελτιωθει η ακριβεια μετα απο καποιες εποχες. Αν οχι, επιστρεφουμε την καλυτερη\n",
    "                self.weights = best_weights\n",
    "                break\n",
    "            \n",
    "            iteration += 1 #increasing number of epochs\n",
    "\n",
    "    def predict(self, x_test_binary):\n",
    "        num_of_examples = x_test_binary.shape[0]\n",
    "        x_test = np.c_[np.ones(num_of_examples), x_test_binary]     #insert 1 for w0\n",
    "\n",
    "        y_pred = list()\n",
    "        for example in x_test:\n",
    "            sign = np.dot(self.weights, example)\n",
    "            # apofasi katataksis diafaneia 4 lecture 18\n",
    "            if sign > 0 : \n",
    "                y_pred.append(1)\n",
    "            else:\n",
    "                y_pred.append(0)\n",
    "\n",
    "        return np.array(y_pred)\n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training And Testing Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom logistic regression with sga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.86     12500\n",
      "           1       0.85      0.87      0.86     12500\n",
      "\n",
      "    accuracy                           0.86     25000\n",
      "   macro avg       0.86      0.86      0.86     25000\n",
      "weighted avg       0.86      0.86      0.86     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Algorithm to use in the optimization problem.\n",
    "#Each solver tries to find the parameter weights that minimize a cost function\n",
    "log_c = CustomLogisticRegression(0.00500000000001, 0.001, 200)\n",
    "log_c.fit(x_train_binary, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, log_c.predict(x_test_binary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87     12500\n",
      "           1       0.87      0.88      0.87     12500\n",
      "\n",
      "    accuracy                           0.87     25000\n",
      "   macro avg       0.87      0.87      0.87     25000\n",
      "weighted avg       0.87      0.87      0.87     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Algorithm to use in the optimization problem.\n",
    "#Each solver tries to find the parameter weights that minimize a cost function\n",
    "log = LogisticRegression()\n",
    "log.fit(x_train_binary, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, log.predict(x_test_binary)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
