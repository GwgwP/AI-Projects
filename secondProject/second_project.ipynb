{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching data from imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_fetch():\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()\n",
    "\n",
    "    word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "    index2word = dict((i + 3, word) for (word, i) in word_index.items())\n",
    "    index2word[0] = '[pad]' #padding\n",
    "    index2word[1] = '[bos]' #begin of sentence\n",
    "    index2word[2] = '[oov]' # out of vocabulary\n",
    "    x_train = np.array([' '.join([index2word[idx] for idx in text]) for text in x_train])\n",
    "    x_test = np.array([' '.join([index2word[idx] for idx in text]) for text in x_test])\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vectorize_examples(vocabulary, x_train):\n",
    "#     binary_vectorizer = CountVectorizer(binary=True, vocabulary=vocabulary.keys())\n",
    "#     x_train_binary = binary_vectorizer.fit_transform(x_train)\n",
    "#     x_train_binary = x_train_binary.toarray()\n",
    "#     return x_train_binary\n",
    "\n",
    "def vectorize_examples(vocabulary, x_train):\n",
    "    binary_vectorizer = CountVectorizer(binary=True, vocabulary=vocabulary.keys())\n",
    "    x_train_binary = binary_vectorizer.fit_transform(x_train)\n",
    "    x_train_binary = x_train_binary.toarray()\n",
    "    return x_train_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ig(classes_vector, feature):\n",
    "        classes = set(classes_vector)\n",
    "\n",
    "        HC = 0\n",
    "        for c in classes:\n",
    "            PC = list(classes_vector).count(c) / len(classes_vector)  # P(C=c)\n",
    "            HC += - PC * math.log(PC, 2)  # H(C)\n",
    "            # print('Overall Entropy:', HC)  # entropy for C variable\n",
    "\n",
    "        feature_values = set(feature)  # 0 or 1 in this example\n",
    "        HC_feature = 0\n",
    "        for value in feature_values:\n",
    "            # pf --> P(X=x)\n",
    "            pf = list(feature).count(value) / len(feature)  # count occurences of value \n",
    "            indices = [i for i in range(len(feature)) if feature[i] == value]  # rows (examples) that have X=x\n",
    "\n",
    "            classes_of_feat = [classes_vector[i] for i in indices]  # category of examples listed in indices above\n",
    "            for c in classes:\n",
    "                # pcf --> P(C=c|X=x)\n",
    "                pcf = classes_of_feat.count(c) / len(classes_of_feat)  # given X=x, count C\n",
    "                if pcf != 0: \n",
    "                    # - P(X=x) * P(C=c|X=x) * log2(P(C=c|X=x))\n",
    "                    temp_H = - pf * pcf * math.log(pcf, 2)\n",
    "                    # sum for all values of C (class) and X (values of specific feature)\n",
    "                    HC_feature += temp_H\n",
    "\n",
    "        ig = HC - HC_feature\n",
    "        return ig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_vocabulary(x_train,y_train, n, k, m, l):\n",
    "    words_frequency_dict = dict()\n",
    "\n",
    "    for review in x_train:\n",
    "        distinct_words = set(review.split())\n",
    "\n",
    "        for word in distinct_words:\n",
    "            if word in words_frequency_dict:\n",
    "                words_frequency_dict[word] += 1\n",
    "            else:\n",
    "                words_frequency_dict[word] = 1\n",
    "    \n",
    "    # Remove specific words from the dictionary\n",
    "    for special_word in ['[bos]', '[pad]', '[oov]']:\n",
    "        words_frequency_dict.pop(special_word, None)\n",
    "   \n",
    "\n",
    "\n",
    "    # Sort words based on their frequency in descending order\n",
    "    remaining_words = sorted(words_frequency_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    # Exclude the top n and bottom k words\n",
    "    remaining_words = remaining_words[n:-k] \n",
    "    # Convert remaining_words back into a dictionary\n",
    "    remaining_words = dict(remaining_words)\n",
    "\n",
    "    # Create a new dictionary which shows the IG\n",
    "    IG_dict = dict()\n",
    "    x_train_binary = vectorize_examples(remaining_words, x_train)\n",
    "    \n",
    "\n",
    "    for i in tqdm(range(len(remaining_words))):\n",
    "        # word = [example[i] for example in x_train_binary.T]\n",
    "        word = [example[i] for example in x_train_binary]\n",
    "        IG_dict[list(remaining_words.keys())[i]] = calculate_ig(y_train, word)\n",
    "\n",
    "    # Sort words based on Information Gain in ascending order\n",
    "    remaining_words = sorted(IG_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    # Select the top l words\n",
    "    remaining_words = remaining_words[:l] \n",
    "    # Convert remaining_words back into a dictionary\n",
    "    remaining_words_dict = dict(remaining_words)\n",
    "\n",
    "    return remaining_words_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3026/3026 [00:32<00:00, 94.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 1000)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = data_fetch()\n",
    "vocabulary = create_vocabulary(x_train,y_train, 50, 85000, 2500, 1000)\n",
    "x_train_binary = vectorize_examples(vocabulary, x_train)\n",
    "x_test_binary = vectorize_examples(vocabulary, x_test)\n",
    "print(x_train_binary.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesCustom():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.class0_prob = None\n",
    "        self.class1_prob = None\n",
    "        self.features_probs = None\n",
    "\n",
    "    def fit(self,x_train_binary, y_train):\n",
    "\n",
    "        # Calculate prior probabilites P(C=0) and P(C=1)\n",
    "        total_samples = len(y_train)\n",
    "        class0_samples = np.sum(y_train == 0)\n",
    "        class0_prob = class0_samples / total_samples\n",
    "        class1_prob = (total_samples - class0_samples) / total_samples\n",
    "\n",
    "        self.class0_prob = class0_prob\n",
    "        self.class1_prob = class1_prob\n",
    "\n",
    "        # Calculate the likelihood\n",
    "        self.feature_probs = np.zeros((2,x_train_binary.shape[1]))\n",
    "\n",
    "        # Select samples belonging to class 0,1\n",
    "        X_0 = []\n",
    "        X_1 = []  #alliws np.array\n",
    "\n",
    "        for i in range(x_train_binary.shape[1]):\n",
    "            if y_train[i] == 0:\n",
    "                X_0.append(x_train_binary[i])\n",
    "            else:\n",
    "                X_1.append(x_train_binary[i])\n",
    "            \n",
    "        # Convert lists to numpy arrays\n",
    "        X_0 = np.array(X_0)\n",
    "        X_1 = np.array(X_1)\n",
    "\n",
    "        # Calculate the probability of each feature being 0 given the class\n",
    "        self.feature_probs[0] = (X_0.sum(axis=0) + 1) / (len(X_0) + 2)   \n",
    "        # Calculate the probability of each feature being 1 given the class\n",
    "        self.feature_probs[1] = (X_1.sum(axis=0) + 1) / (len(X_1) + 2)   \n",
    "        \n",
    "\n",
    "    def predict(self, x_test_binary):\n",
    "        \n",
    "        sum_prob0=0\n",
    "        sum_prob1=0\n",
    "\n",
    "        num_features = x_test_binary.shape[1]\n",
    "        y_predict =[]\n",
    "\n",
    "        # Calculating P(C=1 | x_test_binary) and P(C=0 | x_test_binary)\n",
    "        \n",
    "        for x_test in x_test_binary:\n",
    "            # sum_prob0 = sum( math.log(self.feature_probs[0][i]) if x_test[i] == 1 else  math.log(1-self.feature_probs[0][i]) for i in range(num_features) )\n",
    "            # sum_prob1 = sum( math.log(self.feature_probs[1][i]) if x_test[i] == 1 else  math.log(1-self.feature_probs[1][i]) for i in range(num_features) )\n",
    "\n",
    "            # sum_prob1 = math.log(self.class1_prob) + sum_prob1\n",
    "            # sum_prob0 = math.log(self.class0_prob) + sum_prob0\n",
    "\n",
    "            feature_prob_0 = np.log(self.feature_probs[0])\n",
    "            feature_prob_1 = np.log(self.feature_probs[1])\n",
    "            feature_prob_0 = np.sum(feature_prob_0 * x_test + np.log(1 - np.exp(feature_prob_0) * x_test), axis=0)\n",
    "            feature_prob_1 = np.sum(feature_prob_1 * x_test + np.log(1 - np.exp(feature_prob_1) * x_test), axis=0)\n",
    "            sum_prob0 = np.log(self.class0_prob) + feature_prob_0\n",
    "            sum_prob1 = np.log(self.class1_prob) +feature_prob_1\n",
    "\n",
    "            if (sum_prob1 > sum_prob0):\n",
    "                y_predict.append(1)\n",
    "            elif(sum_prob1 < sum_prob0):\n",
    "                y_predict.append(0)\n",
    "            else:\n",
    "                y_predict.append(1 if self.class1_prob > self.class0_prob else 0)\n",
    "\n",
    "\n",
    "        return y_predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training And Testing - Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                    # 1. Custom Naive Bayes\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85     12500\n",
      "           1       0.87      0.81      0.84     12500\n",
      "\n",
      "    accuracy                           0.85     25000\n",
      "   macro avg       0.85      0.85      0.84     25000\n",
      "weighted avg       0.85      0.85      0.84     25000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.85     12500\n",
      "           1       0.87      0.80      0.83     12500\n",
      "\n",
      "    accuracy                           0.84     25000\n",
      "   macro avg       0.84      0.84      0.84     25000\n",
      "weighted avg       0.84      0.84      0.84     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nbc = NaiveBayesCustom()\n",
    "nbc.fit(x_train_binary, y_train)\n",
    "print(classification_report(y_train, nbc.predict(x_train_binary),zero_division=1))\n",
    "print(classification_report(y_test, nbc.predict(x_test_binary), zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                        # 2. Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84     12500\n",
      "           1       0.85      0.82      0.83     12500\n",
      "\n",
      "    accuracy                           0.84     25000\n",
      "   macro avg       0.84      0.84      0.84     25000\n",
      "weighted avg       0.84      0.84      0.84     25000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.84      0.82     12500\n",
      "           1       0.84      0.80      0.82     12500\n",
      "\n",
      "    accuracy                           0.82     25000\n",
      "   macro avg       0.82      0.82      0.82     25000\n",
      "weighted avg       0.82      0.82      0.82     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(x_train_binary, y_train)\n",
    "print(classification_report(y_train, nb.predict(x_train_binary),\n",
    "                            zero_division=1))\n",
    "print(classification_report(y_test, nb.predict(x_test_binary),\n",
    "                            zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, n_iters, learning_rate, regularizator):\n",
    "        self.n_iters = n_iters\n",
    "        self.learing_rate = learning_rate\n",
    "        self.regularizator = regularizator\n",
    "        self.weights = None\n",
    "\n",
    "    def sigmoid(t):\n",
    "        return 1 / (1 + np.exp(-t))\n",
    "    \n",
    "    def fit(self,x_train_binary, y_train):\n",
    "        self.weights = np.random.rand(1, x_train_binary.shape[1])\n",
    "\n",
    "        i=1\n",
    "        s=0\n",
    "\n",
    "        \n",
    "        while (i<200):\n",
    "            #-------SHUFFLE-------------\n",
    "            # Create a permutation index\n",
    "            permutation_index = np.random.permutation(len(y_train))\n",
    "\n",
    "            # Use the permutation index to shuffle both arrays\n",
    "            shuffled_y_train = y_train[permutation_index]\n",
    "            shuffled_x_train_binary = x_train_binary[permutation_index]\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training And Testing Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# #Algorithm to use in the optimization problem.\n",
    "# #Each solver tries to find the parameter weights that minimize a cost function\n",
    "# log = LogisticRegression()\n",
    "# log.fit(x_train_imdb_binary, y_train_imdb)\n",
    "\n",
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_test_imdb, log.predict(x_test_imdb_binary)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
